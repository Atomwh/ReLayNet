{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Lambda \n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Reshape\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import regularizers, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger,EarlyStopping,ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np    \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.misc import imsave\n",
    "from scipy import ndimage, misc\n",
    "from numpy import unravel_index\n",
    "from operator import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for root, dirnames, filenames in os.walk(\"Dataset/Resized_train/\"):\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        image = ndimage.imread(filepath, mode=\"L\")\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 64)\n"
     ]
    }
   ],
   "source": [
    "print (images[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.load('resized_cropped_labeledimages.npy')\n",
    "labels_list = []\n",
    "for i in range(len(labels)):\n",
    "    labels_list.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 64)\n"
     ]
    }
   ],
   "source": [
    "print(labels_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.zeros((770,216,64,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels_list)) :\n",
    "    for j in range(216) :\n",
    "        for k in range(64):\n",
    "            if(labels_list[i][j][k] == 0):\n",
    "                train_labels[i][j][k][0] = 1\n",
    "            if(labels_list[i][j][k] == 1):\n",
    "                train_labels[i][j][k][1] = 1\n",
    "            if(labels_list[i][j][k] == 2):\n",
    "                train_labels[i][j][k][2] = 1\n",
    "            if(labels_list[i][j][k] == 3):\n",
    "                train_labels[i][j][k][3] = 1\n",
    "            if(labels_list[i][j][k] == 4):\n",
    "                train_labels[i][j][k][4] = 1\n",
    "            if(labels_list[i][j][k] == 5):\n",
    "                train_labels[i][j][k][5] = 1\n",
    "            if(labels_list[i][j][k] == 6):\n",
    "                train_labels[i][j][k][6] = 1\n",
    "            if(labels_list[i][j][k] == 7):\n",
    "                train_labels[i][j][k][7] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770, 216, 64, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770\n",
      "(216, 64, 1)\n",
      "[0, 2, 3, 4, 5, 6, 7, 10, 12, 16, 19, 20, 21, 23, 24, 26, 29, 32, 35, 36, 37, 38, 39, 40, 42, 44, 45, 47, 48, 49, 50, 51, 53, 56, 57, 59, 60, 61, 62, 65, 66, 67, 72, 73, 75, 76, 77, 78, 80, 81, 83, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 105, 107, 109, 111, 114, 115, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 135, 137, 139, 141, 142, 143, 144, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 162, 163, 164, 165, 166, 167, 168, 171, 172, 173, 174, 177, 178, 180, 182, 184, 185, 186, 187, 189, 190, 191, 193, 194, 196, 197, 198, 200, 204, 205, 206, 209, 210, 212, 213, 214, 215, 216, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 230, 233, 234, 235, 238, 239, 240, 243, 248, 252, 255, 256, 258, 260, 261, 263, 264, 265, 266, 268, 269, 270, 273, 274, 276, 277, 279, 285, 286, 287, 288, 289, 290, 291, 292, 296, 297, 298, 300, 303, 304, 305, 306, 308, 309, 311, 312, 314, 316, 317, 319, 320, 322, 323, 325, 331, 333, 334, 335, 336, 338, 339, 340, 342, 343, 344, 347, 350, 351, 354, 355, 356, 357, 358, 360, 362, 363, 365, 366, 369, 370, 371, 375, 376, 378, 380, 381, 382, 383, 384, 386, 387, 389, 392, 393, 394, 395, 396, 400, 402, 405, 406, 408, 409, 410, 414, 416, 417, 418, 419, 422, 424, 425, 426, 427, 429, 433, 434, 436, 437, 441, 442, 443, 444, 445, 447, 448, 449, 450, 452, 453, 454, 455, 456, 458, 459, 460, 463, 466, 467, 469, 470, 471, 472, 473, 475, 476, 479, 480, 483, 486, 487, 488, 490, 493, 494, 495, 496, 497, 501, 502, 503, 505, 506, 507, 509, 511, 512, 514, 515, 516, 517, 518, 519, 520, 521, 522, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 537, 539, 540, 541, 543, 546, 549, 550, 551, 552, 553, 554, 559, 560, 561, 564, 565, 568, 570, 571, 575, 577, 578, 579, 580, 581, 582, 583, 586, 587, 588, 590, 591, 592, 593, 594, 595, 597, 598, 599, 602, 603, 604, 607, 608, 609, 610, 611, 612, 614, 615, 616, 617, 619, 620, 621, 623, 624, 625, 626, 627, 630, 631, 632, 633, 634, 635, 637, 639, 640, 641, 642, 644, 645, 647, 648, 649, 651, 652, 653, 654, 655, 656, 657, 658, 659, 661, 662, 663, 664, 665, 666, 668, 670, 671, 672, 673, 674, 676, 677, 678, 679, 681, 683, 687, 689, 690, 691, 695, 696, 697, 699, 701, 704, 705, 707, 708, 709, 710, 711, 712, 713, 717, 718, 719, 721, 722, 723, 724, 725, 728, 729, 730, 732, 733, 735, 736, 737, 738, 739, 740, 741, 742, 744, 746, 748, 749, 750, 751, 754, 757, 758, 759, 760, 761, 764, 765, 768, 769]\n",
      "[1, 8, 9, 11, 13, 14, 15, 17, 18, 22, 25, 27, 28, 30, 31, 33, 34, 41, 43, 46, 52, 54, 55, 58, 63, 64, 68, 69, 70, 71, 74, 79, 82, 84, 86, 96, 102, 103, 106, 108, 110, 112, 113, 116, 119, 120, 125, 126, 127, 128, 129, 132, 136, 138, 140, 146, 160, 161, 169, 170, 175, 176, 179, 181, 183, 188, 192, 195, 199, 201, 202, 203, 207, 208, 211, 217, 218, 228, 231, 232, 236, 237, 241, 242, 244, 245, 246, 247, 249, 250, 251, 253, 254, 257, 259, 262, 267, 271, 272, 275, 278, 280, 281, 282, 283, 284, 293, 294, 295, 299, 301, 302, 307, 310, 313, 315, 318, 321, 324, 326, 327, 328, 329, 330, 332, 337, 341, 345, 346, 348, 349, 352, 353, 359, 361, 364, 367, 368, 372, 373, 374, 377, 379, 385, 388, 390, 391, 397, 398, 399, 401, 403, 404, 407, 411, 412, 413, 415, 420, 421, 423, 428, 430, 431, 432, 435, 438, 439, 440, 446, 451, 457, 461, 462, 464, 465, 468, 474, 477, 478, 481, 482, 484, 485, 489, 491, 492, 498, 499, 500, 504, 508, 510, 513, 523, 536, 538, 542, 544, 545, 547, 548, 555, 556, 557, 558, 562, 563, 566, 567, 569, 572, 573, 574, 576, 584, 585, 589, 596, 600, 601, 605, 606, 613, 618, 622, 628, 629, 636, 638, 643, 646, 650, 660, 667, 669, 675, 680, 682, 684, 685, 686, 688, 692, 693, 694, 698, 700, 702, 703, 706, 714, 715, 716, 720, 726, 727, 731, 734, 743, 745, 747, 752, 753, 755, 756, 762, 763, 766, 767]\n"
     ]
    }
   ],
   "source": [
    "images=np.array(images)\n",
    "print(images.shape[0])\n",
    "images = images.reshape(images.shape[0],216,64,1)\n",
    "\n",
    "print(images[0].shape)\n",
    "train_indices = np.random.choice(770,500,replace = False)\n",
    "print(sorted(train_indices))\n",
    "train_images_random = []\n",
    "train_labels_random = []\n",
    "\n",
    "for i in train_indices:\n",
    "    train_images_random.append(images[i])\n",
    "    train_labels_random.append(train_labels[i])\n",
    "\n",
    "test_indices = [x for x in range(770) if x not in train_indices]\n",
    "print(test_indices)\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for i in test_indices:\n",
    "    test_images.append(images[i])\n",
    "    test_labels.append(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = np.array(train_images_random)\n",
    "train_labels = np.array(train_labels_random)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32')\n",
    "train_labels = train_labels.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "test_labels = test_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 216, 64, 1)\n",
      "(270, 216, 64, 1)\n",
      "(500, 216, 64, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 35.]\n",
      "  [ 44.]\n",
      "  [ 38.]\n",
      "  ..., \n",
      "  [ 41.]\n",
      "  [ 45.]\n",
      "  [ 36.]]\n",
      "\n",
      " [[ 22.]\n",
      "  [ 26.]\n",
      "  [ 21.]\n",
      "  ..., \n",
      "  [ 50.]\n",
      "  [ 52.]\n",
      "  [ 26.]]\n",
      "\n",
      " [[  0.]\n",
      "  [  0.]\n",
      "  [  0.]\n",
      "  ..., \n",
      "  [ 48.]\n",
      "  [ 45.]\n",
      "  [ 13.]]\n",
      "\n",
      " ..., \n",
      " [[ 60.]\n",
      "  [ 60.]\n",
      "  [ 57.]\n",
      "  ..., \n",
      "  [ 73.]\n",
      "  [ 72.]\n",
      "  [ 65.]]\n",
      "\n",
      " [[ 60.]\n",
      "  [ 52.]\n",
      "  [ 47.]\n",
      "  ..., \n",
      "  [ 72.]\n",
      "  [ 68.]\n",
      "  [ 62.]]\n",
      "\n",
      " [[ 60.]\n",
      "  [ 49.]\n",
      "  [ 44.]\n",
      "  ..., \n",
      "  [ 67.]\n",
      "  [ 63.]\n",
      "  [ 66.]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "(500, 216, 64, 8)\n"
     ]
    }
   ],
   "source": [
    "print (len(train_images))\n",
    "print (len(train_labels))\n",
    "print (np.array(train_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 216, 64, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_shape = 216*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 216, 64, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 216, 64, 64)       640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 216, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 216, 64, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 108, 32, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 108, 32, 64)       36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 108, 32, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 108, 32, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 54, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 54, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 54, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 27, 8, 64)         36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 27, 8, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 27, 8, 64)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 54, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_1 (Concatenate)  (None, 54, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 54, 16, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 54, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 54, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 108, 32, 64)       0         \n",
      "_________________________________________________________________\n",
      "concatenate_2 (Concatenate)  (None, 108, 32, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 108, 32, 64)       73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 108, 32, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 108, 32, 64)       0         \n",
      "_________________________________________________________________\n",
      "Layer19 (UpSampling2D)       (None, 216, 64, 64)       0         \n",
      "_________________________________________________________________\n",
      "concatenate_3 (Concatenate)  (None, 216, 64, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 216, 64, 64)       73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 216, 64, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 216, 64, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 216, 64, 8)        520       \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 13824, 8)          0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 13824, 8)          0         \n",
      "=================================================================\n",
      "Total params: 335,112.0\n",
      "Trainable params: 334,216.0\n",
      "Non-trainable params: 896.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defines the input tensor\n",
    "inputs = Input(shape=(216,64,1))\n",
    "\n",
    "L1 = Conv2D(64,kernel_size=(3,3),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(inputs)\n",
    "L2 = BatchNormalization()(L1)\n",
    "L2 = Activation('relu')(L2)\n",
    "#L3 = Lambda(maxpool_1,output_shape = shape)(L2)\n",
    "L3 = MaxPooling2D(pool_size=(2,2))(L2)\n",
    "L4 = Conv2D(64,kernel_size=(3,3),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L3)\n",
    "L5 = BatchNormalization()(L4)\n",
    "L5 = Activation('relu')(L5)\n",
    "#L6 = Lambda(maxpool_2,output_shape = shape)(L5)\n",
    "L6 = MaxPooling2D(pool_size=(2,2))(L5)\n",
    "L7 = Conv2D(64,kernel_size=(3,3),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L6)\n",
    "L8 = BatchNormalization()(L7)\n",
    "L8 = Activation('relu')(L8)\n",
    "#L9 = Lambda(maxpool_3,output_shape = shape)(L8)\n",
    "L9 = MaxPooling2D(pool_size=(2,2))(L8)\n",
    "L10 = Conv2D(64,kernel_size=(3,3),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L9)\n",
    "L11 = BatchNormalization()(L10)\n",
    "L11 = Activation('relu')(L11)\n",
    "L12 = UpSampling2D(size = (2,2))(L11)\n",
    "#L12 = Lambda(unpool_3,output_shape = unpool_shape)(L11)\n",
    "L13 = Concatenate(axis = 3)([L8,L12])\n",
    "L14 = Conv2D(64,kernel_size=(3,3),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L13)\n",
    "L15 = BatchNormalization()(L14)\n",
    "L15 = Activation('relu')(L15)\n",
    "L16 = UpSampling2D(size= (2,2))(L15)\n",
    "#L16 = Lambda(unpool_2,output_shape=unpool_shape)(L15)\n",
    "L17 = Concatenate(axis = 3)([L16,L5])\n",
    "L18 = Conv2D(64,kernel_size=(3,3),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L17)\n",
    "L19 = BatchNormalization()(L18)\n",
    "L19 = Activation('relu')(L19)\n",
    "#L20 = Lambda(unpool_1,output_shape=unpool_shape)(L19)\n",
    "L20 = UpSampling2D(size=(2,2),name = \"Layer19\")(L19)\n",
    "L21 = Concatenate(axis=3)([L20,L2])\n",
    "L22 = Conv2D(64,kernel_size=(3,3),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L21)\n",
    "L23 = BatchNormalization()(L22)\n",
    "L23 = Activation('relu')(L23)\n",
    "L24 = Conv2D(8,kernel_size=(1,1),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L23)\n",
    "L = Reshape((data_shape,8),input_shape = (216,64,8))(L24)\n",
    "L = Activation('softmax')(L)\n",
    "model = Model(inputs = inputs, outputs = L)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.load('weighted_cropped_images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(770, 216, 64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.   ,   4.336,   6.63 ,  12.007,  12.459,  14.647,  15.368,\n",
       "        17.978,  19.336,  21.63 ,  27.007,  27.459,  29.647,  30.368,\n",
       "        32.978])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_matrix = []\n",
    "for i in train_indices:\n",
    "    weights_matrix.append(weights[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights = np.array(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights = np.reshape(sample_weights,(500,data_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 13824)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 216, 64, 8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.reshape(train_labels,(500,data_shape,8))\n",
    "test_labels = np.reshape(test_labels,(270,data_shape,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 13824, 8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = np.zeros(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    for j in range(13824):\n",
    "        for k in range(8):\n",
    "            if(train_labels[i][j][k]==1):\n",
    "                count[k] = count[k]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3972411.0\n",
      "312782.0\n",
      "605075.0\n",
      "341224.0\n",
      "263099.0\n",
      "920815.0\n",
      "274885.0\n",
      "221709.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3972411.,   312782.,   605075.,   341224.,   263099.,   920815.,\n",
       "         274885.,   221709.])"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "median = np.median(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327003.0"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = np.zeros(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    scale[i] = (median/count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08176891,  1.01882538,  0.54272503,  0.98185769,  1.25668549,\n",
       "        0.354605  ,  1.19773269,  1.47007454])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.zeros(8)\n",
    "for i in range(8):\n",
    "    weights[i] = scale[i]/scale[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.        ,  12.45981304,   6.63730261,  12.00771353,\n",
       "        15.36874386,   4.33667245,  14.6477755 ,  17.97840371])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customized_loss(y_true,y_pred):\n",
    "    return (1*K.categorical_crossentropy(y_true, y_pred))+(0.5*dice_coef_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimiser = optimizers.Adam(lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimiser,loss=customized_loss,metrics=['accuracy',dice_coef],sample_weight_mode='temporal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 64, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining Callback functions which will be called by model during runtime when specified condition satisfies\n",
    "lr_reducer = ReduceLROnPlateau(factor=0.5, cooldown=0, patience=6, min_lr=0.5e-6)\n",
    "csv_logger = CSVLogger('Relaynet_sample_weights5.csv')\n",
    "model_chekpoint = ModelCheckpoint(\"Relaynet_sample_weights5.hdf5\",monitor = 'val_loss',verbose = 1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples, validate on 270 samples\n",
      "Epoch 1/200\n",
      "480/500 [===========================>..] - ETA: 2s - loss: 67.6555 - acc: 0.4213 - dice_coef: 0.3711Epoch 00000: val_loss improved from inf to 15.28306, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 64s - loss: 67.3559 - acc: 0.4229 - dice_coef: 0.3733 - val_loss: 15.2831 - val_acc: 0.0517 - val_dice_coef: 0.0531\n",
      "Epoch 2/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 62.3662 - acc: 0.4494 - dice_coef: 0.4260Epoch 00001: val_loss improved from 15.28306 to 13.74751, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 62.2564 - acc: 0.4505 - dice_coef: 0.4273 - val_loss: 13.7475 - val_acc: 0.1853 - val_dice_coef: 0.1456\n",
      "Epoch 3/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 60.1404 - acc: 0.4415 - dice_coef: 0.4294Epoch 00002: val_loss improved from 13.74751 to 11.16490, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 60.2845 - acc: 0.4396 - dice_coef: 0.4278 - val_loss: 11.1649 - val_acc: 0.3235 - val_dice_coef: 0.3011\n",
      "Epoch 4/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 59.8650 - acc: 0.4308 - dice_coef: 0.4246Epoch 00003: val_loss improved from 11.16490 to 10.87756, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 59.7140 - acc: 0.4319 - dice_coef: 0.4257 - val_loss: 10.8776 - val_acc: 0.3317 - val_dice_coef: 0.3184\n",
      "Epoch 5/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 59.5996 - acc: 0.4300 - dice_coef: 0.4248Epoch 00004: val_loss improved from 10.87756 to 9.93065, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 59.5044 - acc: 0.4296 - dice_coef: 0.4244 - val_loss: 9.9306 - val_acc: 0.3863 - val_dice_coef: 0.3754\n",
      "Epoch 6/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 58.9265 - acc: 0.4198 - dice_coef: 0.4161Epoch 00005: val_loss improved from 9.93065 to 9.14896, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 58.8895 - acc: 0.4188 - dice_coef: 0.4151 - val_loss: 9.1490 - val_acc: 0.4307 - val_dice_coef: 0.4225\n",
      "Epoch 7/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 58.3616 - acc: 0.4199 - dice_coef: 0.4168Epoch 00006: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 58.2464 - acc: 0.4201 - dice_coef: 0.4170 - val_loss: 11.0228 - val_acc: 0.3247 - val_dice_coef: 0.3097\n",
      "Epoch 8/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 58.1843 - acc: 0.4236 - dice_coef: 0.4207Epoch 00007: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 58.2491 - acc: 0.4239 - dice_coef: 0.4210 - val_loss: 11.2553 - val_acc: 0.3068 - val_dice_coef: 0.2958\n",
      "Epoch 9/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 57.8807 - acc: 0.4230 - dice_coef: 0.4204Epoch 00008: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 57.6051 - acc: 0.4244 - dice_coef: 0.4218 - val_loss: 9.4359 - val_acc: 0.4084 - val_dice_coef: 0.4053\n",
      "Epoch 10/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 57.5624 - acc: 0.4200 - dice_coef: 0.4181Epoch 00009: val_loss improved from 9.14896 to 9.09512, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 57.4922 - acc: 0.4204 - dice_coef: 0.4185 - val_loss: 9.0951 - val_acc: 0.4276 - val_dice_coef: 0.4258\n",
      "Epoch 11/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 57.7249 - acc: 0.4241 - dice_coef: 0.4215Epoch 00010: val_loss improved from 9.09512 to 9.09494, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 57.8570 - acc: 0.4239 - dice_coef: 0.4213 - val_loss: 9.0949 - val_acc: 0.4273 - val_dice_coef: 0.4259\n",
      "Epoch 12/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 58.0289 - acc: 0.4214 - dice_coef: 0.4199Epoch 00011: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 57.8138 - acc: 0.4221 - dice_coef: 0.4204 - val_loss: 11.7293 - val_acc: 0.2636 - val_dice_coef: 0.2674\n",
      "Epoch 13/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 57.4175 - acc: 0.4269 - dice_coef: 0.4249Epoch 00012: val_loss improved from 9.09494 to 8.93116, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 57.4656 - acc: 0.4254 - dice_coef: 0.4234 - val_loss: 8.9312 - val_acc: 0.4379 - val_dice_coef: 0.4358\n",
      "Epoch 14/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 57.2020 - acc: 0.4183 - dice_coef: 0.4164Epoch 00013: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 57.0644 - acc: 0.4188 - dice_coef: 0.4169 - val_loss: 9.4477 - val_acc: 0.4066 - val_dice_coef: 0.4048\n",
      "Epoch 15/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 57.0710 - acc: 0.4281 - dice_coef: 0.4265Epoch 00014: val_loss improved from 8.93116 to 8.84024, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 57.1774 - acc: 0.4270 - dice_coef: 0.4253 - val_loss: 8.8402 - val_acc: 0.4418 - val_dice_coef: 0.4414\n",
      "Epoch 16/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 57.2943 - acc: 0.4168 - dice_coef: 0.4155Epoch 00015: val_loss improved from 8.84024 to 8.75868, saving model to Relaynet_sample_weights5.hdf5\n",
      "500/500 [==============================] - 52s - loss: 57.1806 - acc: 0.4183 - dice_coef: 0.4170 - val_loss: 8.7587 - val_acc: 0.4465 - val_dice_coef: 0.4463\n",
      "Epoch 17/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 56.5020 - acc: 0.4254 - dice_coef: 0.4242Epoch 00016: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 56.6540 - acc: 0.4253 - dice_coef: 0.4242 - val_loss: 9.0978 - val_acc: 0.4259 - val_dice_coef: 0.4259\n",
      "Epoch 18/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 56.7997 - acc: 0.4220 - dice_coef: 0.4206Epoch 00017: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 56.7562 - acc: 0.4227 - dice_coef: 0.4213 - val_loss: 9.2232 - val_acc: 0.4213 - val_dice_coef: 0.4184\n",
      "Epoch 19/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 55.9671 - acc: 0.4284 - dice_coef: 0.4273Epoch 00018: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 56.2189 - acc: 0.4283 - dice_coef: 0.4272 - val_loss: 9.3552 - val_acc: 0.4109 - val_dice_coef: 0.4105\n",
      "Epoch 20/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 55.7321 - acc: 0.4315 - dice_coef: 0.4299Epoch 00019: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 55.7457 - acc: 0.4324 - dice_coef: 0.4308 - val_loss: 8.8212 - val_acc: 0.4436 - val_dice_coef: 0.4427\n",
      "Epoch 21/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 55.1098 - acc: 0.4312 - dice_coef: 0.4297Epoch 00020: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 55.1727 - acc: 0.4312 - dice_coef: 0.4297 - val_loss: 9.1142 - val_acc: 0.4257 - val_dice_coef: 0.4251\n",
      "Epoch 22/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 55.9827 - acc: 0.4324 - dice_coef: 0.4313Epoch 00021: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 55.7641 - acc: 0.4326 - dice_coef: 0.4315 - val_loss: 9.3712 - val_acc: 0.4114 - val_dice_coef: 0.4097\n",
      "Epoch 23/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 54.8664 - acc: 0.4378 - dice_coef: 0.4366Epoch 00022: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 55.0901 - acc: 0.4365 - dice_coef: 0.4353 - val_loss: 9.2828 - val_acc: 0.4154 - val_dice_coef: 0.4151\n",
      "Epoch 24/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 54.1920 - acc: 0.4322 - dice_coef: 0.4312Epoch 00023: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 54.5244 - acc: 0.4315 - dice_coef: 0.4305 - val_loss: 9.1198 - val_acc: 0.4257 - val_dice_coef: 0.4249\n",
      "Epoch 25/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 53.9082 - acc: 0.4418 - dice_coef: 0.4407Epoch 00024: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 53.6770 - acc: 0.4426 - dice_coef: 0.4416 - val_loss: 9.3598 - val_acc: 0.4111 - val_dice_coef: 0.4105\n",
      "Epoch 26/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 53.1936 - acc: 0.4399 - dice_coef: 0.4384Epoch 00025: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 53.0083 - acc: 0.4399 - dice_coef: 0.4383 - val_loss: 9.1531 - val_acc: 0.4228 - val_dice_coef: 0.4229\n",
      "Epoch 27/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 52.8629 - acc: 0.4478 - dice_coef: 0.4468Epoch 00026: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 52.8763 - acc: 0.4476 - dice_coef: 0.4465 - val_loss: 9.1833 - val_acc: 0.4218 - val_dice_coef: 0.4211\n",
      "Epoch 28/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 52.1600 - acc: 0.4481 - dice_coef: 0.4468Epoch 00027: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 52.3599 - acc: 0.4475 - dice_coef: 0.4462 - val_loss: 9.1377 - val_acc: 0.4244 - val_dice_coef: 0.4239\n",
      "Epoch 29/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 52.0332 - acc: 0.4486 - dice_coef: 0.4471Epoch 00028: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 52.0932 - acc: 0.4491 - dice_coef: 0.4477 - val_loss: 8.8124 - val_acc: 0.4445 - val_dice_coef: 0.4434\n",
      "Epoch 30/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 50.7843 - acc: 0.4558 - dice_coef: 0.4542Epoch 00029: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 50.9163 - acc: 0.4565 - dice_coef: 0.4549 - val_loss: 9.2850 - val_acc: 0.4156 - val_dice_coef: 0.4150\n",
      "Epoch 31/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 50.2160 - acc: 0.4589 - dice_coef: 0.4573Epoch 00030: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 50.2931 - acc: 0.4592 - dice_coef: 0.4576 - val_loss: 8.9379 - val_acc: 0.4368 - val_dice_coef: 0.4359\n",
      "Epoch 32/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 49.7127 - acc: 0.4682 - dice_coef: 0.4665Epoch 00031: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 49.7199 - acc: 0.4681 - dice_coef: 0.4665 - val_loss: 9.3017 - val_acc: 0.4147 - val_dice_coef: 0.4140\n",
      "Epoch 33/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 49.4189 - acc: 0.4694 - dice_coef: 0.4679Epoch 00032: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 49.4308 - acc: 0.4687 - dice_coef: 0.4671 - val_loss: 9.1197 - val_acc: 0.4258 - val_dice_coef: 0.4250\n",
      "Epoch 34/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 49.1416 - acc: 0.4672 - dice_coef: 0.4653Epoch 00033: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 49.0648 - acc: 0.4676 - dice_coef: 0.4658 - val_loss: 9.1431 - val_acc: 0.4244 - val_dice_coef: 0.4236\n",
      "Epoch 35/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 48.8264 - acc: 0.4700 - dice_coef: 0.4682Epoch 00034: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 48.8811 - acc: 0.4699 - dice_coef: 0.4681 - val_loss: 9.3053 - val_acc: 0.4145 - val_dice_coef: 0.4138\n",
      "Epoch 36/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 47.7506 - acc: 0.4770 - dice_coef: 0.4751Epoch 00035: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 47.7038 - acc: 0.4774 - dice_coef: 0.4755 - val_loss: 8.9743 - val_acc: 0.4347 - val_dice_coef: 0.4337\n",
      "Epoch 37/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 47.1305 - acc: 0.4821 - dice_coef: 0.4799Epoch 00036: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 47.3545 - acc: 0.4812 - dice_coef: 0.4790 - val_loss: 9.0125 - val_acc: 0.4323 - val_dice_coef: 0.4314\n",
      "Epoch 38/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 47.0837 - acc: 0.4800 - dice_coef: 0.4778Epoch 00037: val_loss did not improve\n",
      "500/500 [==============================] - 53s - loss: 47.1045 - acc: 0.4790 - dice_coef: 0.4767 - val_loss: 8.7856 - val_acc: 0.4463 - val_dice_coef: 0.4451\n",
      "Epoch 39/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 46.9735 - acc: 0.4839 - dice_coef: 0.4817Epoch 00038: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 46.8851 - acc: 0.4847 - dice_coef: 0.4825 - val_loss: 9.0393 - val_acc: 0.4310 - val_dice_coef: 0.4298\n",
      "Epoch 40/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 46.7089 - acc: 0.4855 - dice_coef: 0.4833Epoch 00039: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 46.6959 - acc: 0.4856 - dice_coef: 0.4834 - val_loss: 8.9756 - val_acc: 0.4346 - val_dice_coef: 0.4337\n",
      "Epoch 41/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 46.1725 - acc: 0.4875 - dice_coef: 0.4852Epoch 00040: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 46.2871 - acc: 0.4867 - dice_coef: 0.4844 - val_loss: 9.0886 - val_acc: 0.4278 - val_dice_coef: 0.4269\n",
      "Epoch 42/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 45.8132 - acc: 0.4926 - dice_coef: 0.4903Epoch 00041: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 45.8104 - acc: 0.4925 - dice_coef: 0.4901 - val_loss: 9.0598 - val_acc: 0.4296 - val_dice_coef: 0.4286\n",
      "Epoch 43/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 45.7005 - acc: 0.4872 - dice_coef: 0.4847Epoch 00042: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 45.5971 - acc: 0.4888 - dice_coef: 0.4863 - val_loss: 9.0045 - val_acc: 0.4332 - val_dice_coef: 0.4319\n",
      "Epoch 44/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 45.5729 - acc: 0.4928 - dice_coef: 0.4902Epoch 00043: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 45.5050 - acc: 0.4931 - dice_coef: 0.4905 - val_loss: 9.0535 - val_acc: 0.4304 - val_dice_coef: 0.4290\n",
      "Epoch 45/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 45.3719 - acc: 0.4927 - dice_coef: 0.4902Epoch 00044: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 45.3456 - acc: 0.4934 - dice_coef: 0.4909 - val_loss: 9.1031 - val_acc: 0.4273 - val_dice_coef: 0.4260\n",
      "Epoch 46/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 45.4806 - acc: 0.4924 - dice_coef: 0.4899Epoch 00045: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 45.1330 - acc: 0.4938 - dice_coef: 0.4913 - val_loss: 9.0654 - val_acc: 0.4294 - val_dice_coef: 0.4283\n",
      "Epoch 47/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.9631 - acc: 0.4977 - dice_coef: 0.4951Epoch 00046: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 45.0817 - acc: 0.4965 - dice_coef: 0.4940 - val_loss: 9.0130 - val_acc: 0.4325 - val_dice_coef: 0.4314\n",
      "Epoch 48/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.8407 - acc: 0.4958 - dice_coef: 0.4932Epoch 00047: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.8862 - acc: 0.4957 - dice_coef: 0.4931 - val_loss: 9.0815 - val_acc: 0.4285 - val_dice_coef: 0.4273\n",
      "Epoch 49/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.8099 - acc: 0.4965 - dice_coef: 0.4938Epoch 00048: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.7080 - acc: 0.4973 - dice_coef: 0.4947 - val_loss: 9.0343 - val_acc: 0.4314 - val_dice_coef: 0.4301\n",
      "Epoch 50/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.3764 - acc: 0.4996 - dice_coef: 0.4970Epoch 00049: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.6555 - acc: 0.4988 - dice_coef: 0.4962 - val_loss: 9.0535 - val_acc: 0.4302 - val_dice_coef: 0.4290\n",
      "Epoch 51/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.7436 - acc: 0.4978 - dice_coef: 0.4953Epoch 00050: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.5513 - acc: 0.4987 - dice_coef: 0.4961 - val_loss: 9.0476 - val_acc: 0.4306 - val_dice_coef: 0.4293\n",
      "Epoch 52/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.6662 - acc: 0.5000 - dice_coef: 0.4972Epoch 00051: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.5141 - acc: 0.4997 - dice_coef: 0.4970 - val_loss: 9.1020 - val_acc: 0.4273 - val_dice_coef: 0.4261\n",
      "Epoch 53/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.4993 - acc: 0.4989 - dice_coef: 0.4963Epoch 00052: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.4374 - acc: 0.4994 - dice_coef: 0.4968 - val_loss: 9.0522 - val_acc: 0.4302 - val_dice_coef: 0.4291\n",
      "Epoch 54/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.3815 - acc: 0.4998 - dice_coef: 0.4971Epoch 00053: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.2667 - acc: 0.5000 - dice_coef: 0.4973 - val_loss: 9.0595 - val_acc: 0.4298 - val_dice_coef: 0.4286\n",
      "Epoch 55/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.5924 - acc: 0.4994 - dice_coef: 0.4966Epoch 00054: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.3159 - acc: 0.5009 - dice_coef: 0.4981 - val_loss: 9.0333 - val_acc: 0.4315 - val_dice_coef: 0.4302\n",
      "Epoch 56/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.1558 - acc: 0.5034 - dice_coef: 0.5007Epoch 00055: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.2588 - acc: 0.5021 - dice_coef: 0.4993 - val_loss: 9.0305 - val_acc: 0.4316 - val_dice_coef: 0.4304\n",
      "Epoch 57/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.1971 - acc: 0.5012 - dice_coef: 0.4984Epoch 00056: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.2681 - acc: 0.5008 - dice_coef: 0.4980 - val_loss: 9.0650 - val_acc: 0.4295 - val_dice_coef: 0.4283\n",
      "Epoch 58/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0501 - acc: 0.5017 - dice_coef: 0.4990Epoch 00057: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.1510 - acc: 0.5015 - dice_coef: 0.4988 - val_loss: 9.0459 - val_acc: 0.4307 - val_dice_coef: 0.4294\n",
      "Epoch 59/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.1775 - acc: 0.5015 - dice_coef: 0.4987Epoch 00058: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.1515 - acc: 0.5015 - dice_coef: 0.4987 - val_loss: 9.0791 - val_acc: 0.4288 - val_dice_coef: 0.4274\n",
      "Epoch 60/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0562 - acc: 0.5006 - dice_coef: 0.4979Epoch 00059: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.1337 - acc: 0.5011 - dice_coef: 0.4983 - val_loss: 9.0491 - val_acc: 0.4305 - val_dice_coef: 0.4292\n",
      "Epoch 61/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9101 - acc: 0.5043 - dice_coef: 0.5015Epoch 00060: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.0399 - acc: 0.5030 - dice_coef: 0.5004 - val_loss: 9.0527 - val_acc: 0.4303 - val_dice_coef: 0.4290\n",
      "Epoch 62/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.3310 - acc: 0.5009 - dice_coef: 0.4982Epoch 00061: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.0786 - acc: 0.5023 - dice_coef: 0.4996 - val_loss: 9.0629 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 63/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9838 - acc: 0.5035 - dice_coef: 0.5007Epoch 00062: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.0037 - acc: 0.5031 - dice_coef: 0.5003 - val_loss: 9.0656 - val_acc: 0.4296 - val_dice_coef: 0.4282\n",
      "Epoch 64/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9323 - acc: 0.5035 - dice_coef: 0.5007Epoch 00063: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9608 - acc: 0.5028 - dice_coef: 0.5001 - val_loss: 9.0638 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 65/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9377 - acc: 0.5027 - dice_coef: 0.5000Epoch 00064: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9341 - acc: 0.5030 - dice_coef: 0.5003 - val_loss: 9.0520 - val_acc: 0.4304 - val_dice_coef: 0.4291\n",
      "Epoch 66/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9830 - acc: 0.5028 - dice_coef: 0.4999Epoch 00065: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8799 - acc: 0.5040 - dice_coef: 0.5011 - val_loss: 9.0543 - val_acc: 0.4302 - val_dice_coef: 0.4289\n",
      "Epoch 67/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7585 - acc: 0.5049 - dice_coef: 0.5022Epoch 00066: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.0402 - acc: 0.5028 - dice_coef: 0.5001 - val_loss: 9.0569 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 68/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.3852 - acc: 0.5017 - dice_coef: 0.4989Epoch 00067: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.1106 - acc: 0.5021 - dice_coef: 0.4993 - val_loss: 9.0610 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 69/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0747 - acc: 0.5020 - dice_coef: 0.4992Epoch 00068: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.0045 - acc: 0.5026 - dice_coef: 0.4998 - val_loss: 9.0577 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 70/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8039 - acc: 0.5030 - dice_coef: 0.5004Epoch 00069: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.0429 - acc: 0.5021 - dice_coef: 0.4995 - val_loss: 9.0599 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 71/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9116 - acc: 0.5035 - dice_coef: 0.5007Epoch 00070: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9887 - acc: 0.5024 - dice_coef: 0.4996 - val_loss: 9.0704 - val_acc: 0.4292 - val_dice_coef: 0.4280\n",
      "Epoch 72/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8931 - acc: 0.5032 - dice_coef: 0.5004Epoch 00071: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8951 - acc: 0.5032 - dice_coef: 0.5004 - val_loss: 9.0643 - val_acc: 0.4296 - val_dice_coef: 0.4283\n",
      "Epoch 73/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.1265 - acc: 0.5022 - dice_coef: 0.4995Epoch 00072: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9773 - acc: 0.5028 - dice_coef: 0.5001 - val_loss: 9.0613 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 74/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.1709 - acc: 0.5023 - dice_coef: 0.4997Epoch 00073: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9024 - acc: 0.5031 - dice_coef: 0.5005 - val_loss: 9.0632 - val_acc: 0.4296 - val_dice_coef: 0.4284\n",
      "Epoch 75/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9211 - acc: 0.5036 - dice_coef: 0.5008Epoch 00074: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8530 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0593 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 76/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9251 - acc: 0.5033 - dice_coef: 0.5005Epoch 00075: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8995 - acc: 0.5033 - dice_coef: 0.5005 - val_loss: 9.0598 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 77/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.2562 - acc: 0.5006 - dice_coef: 0.4978Epoch 00076: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 44.0451 - acc: 0.5027 - dice_coef: 0.4999 - val_loss: 9.0601 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 78/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8684 - acc: 0.5043 - dice_coef: 0.5015Epoch 00077: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8725 - acc: 0.5034 - dice_coef: 0.5007 - val_loss: 9.0606 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 79/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6265 - acc: 0.5045 - dice_coef: 0.5017Epoch 00078: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8350 - acc: 0.5040 - dice_coef: 0.5011 - val_loss: 9.0597 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 80/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9249 - acc: 0.5036 - dice_coef: 0.5007Epoch 00079: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9347 - acc: 0.5032 - dice_coef: 0.5003 - val_loss: 9.0631 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 81/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8944 - acc: 0.5039 - dice_coef: 0.5011Epoch 00080: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8245 - acc: 0.5035 - dice_coef: 0.5007 - val_loss: 9.0635 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 82/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.2396 - acc: 0.5010 - dice_coef: 0.4982Epoch 00081: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9771 - acc: 0.5027 - dice_coef: 0.5000 - val_loss: 9.0637 - val_acc: 0.4296 - val_dice_coef: 0.4284\n",
      "Epoch 83/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0351 - acc: 0.5024 - dice_coef: 0.4996Epoch 00082: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9668 - acc: 0.5030 - dice_coef: 0.5002 - val_loss: 9.0622 - val_acc: 0.4297 - val_dice_coef: 0.4285\n",
      "Epoch 84/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0616 - acc: 0.5018 - dice_coef: 0.4990Epoch 00083: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9202 - acc: 0.5030 - dice_coef: 0.5002 - val_loss: 9.0620 - val_acc: 0.4297 - val_dice_coef: 0.4285\n",
      "Epoch 85/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9859 - acc: 0.5019 - dice_coef: 0.4991Epoch 00084: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9709 - acc: 0.5028 - dice_coef: 0.4999 - val_loss: 9.0615 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 86/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6680 - acc: 0.5039 - dice_coef: 0.5011Epoch 00085: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8766 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0627 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 87/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9117 - acc: 0.5031 - dice_coef: 0.5003Epoch 00086: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8776 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0626 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 88/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9311 - acc: 0.5041 - dice_coef: 0.5013Epoch 00087: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8571 - acc: 0.5039 - dice_coef: 0.5011 - val_loss: 9.0630 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 89/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.1568 - acc: 0.5024 - dice_coef: 0.4996Epoch 00088: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7909 - acc: 0.5037 - dice_coef: 0.5009 - val_loss: 9.0625 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 90/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5838 - acc: 0.5043 - dice_coef: 0.5015Epoch 00089: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8531 - acc: 0.5034 - dice_coef: 0.5006 - val_loss: 9.0623 - val_acc: 0.4297 - val_dice_coef: 0.4284\n",
      "Epoch 91/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8708 - acc: 0.5041 - dice_coef: 0.5013Epoch 00090: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8240 - acc: 0.5038 - dice_coef: 0.5010 - val_loss: 9.0620 - val_acc: 0.4297 - val_dice_coef: 0.4285\n",
      "Epoch 92/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7940 - acc: 0.5028 - dice_coef: 0.5001Epoch 00091: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8632 - acc: 0.5034 - dice_coef: 0.5006 - val_loss: 9.0614 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 93/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7511 - acc: 0.5044 - dice_coef: 0.5016Epoch 00092: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9072 - acc: 0.5034 - dice_coef: 0.5007 - val_loss: 9.0608 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 94/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8919 - acc: 0.5031 - dice_coef: 0.5004Epoch 00093: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9734 - acc: 0.5029 - dice_coef: 0.5001 - val_loss: 9.0606 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 95/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9027 - acc: 0.5036 - dice_coef: 0.5008Epoch 00094: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8499 - acc: 0.5034 - dice_coef: 0.5006 - val_loss: 9.0611 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 96/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9219 - acc: 0.5033 - dice_coef: 0.5005Epoch 00095: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8624 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0609 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 97/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.1516 - acc: 0.5024 - dice_coef: 0.4997Epoch 00096: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9884 - acc: 0.5029 - dice_coef: 0.5002 - val_loss: 9.0613 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 98/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0314 - acc: 0.5019 - dice_coef: 0.4991Epoch 00097: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8859 - acc: 0.5035 - dice_coef: 0.5007 - val_loss: 9.0615 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 99/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9376 - acc: 0.5033 - dice_coef: 0.5006Epoch 00098: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8890 - acc: 0.5032 - dice_coef: 0.5005 - val_loss: 9.0611 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 100/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6745 - acc: 0.5042 - dice_coef: 0.5014Epoch 00099: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8275 - acc: 0.5036 - dice_coef: 0.5009 - val_loss: 9.0607 - val_acc: 0.4298 - val_dice_coef: 0.4285\n",
      "Epoch 101/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5548 - acc: 0.5054 - dice_coef: 0.5026Epoch 00100: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7702 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0596 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 102/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9533 - acc: 0.5029 - dice_coef: 0.5001Epoch 00101: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8980 - acc: 0.5034 - dice_coef: 0.5006 - val_loss: 9.0593 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 103/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0626 - acc: 0.5032 - dice_coef: 0.5005Epoch 00102: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9180 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0591 - val_acc: 0.4300 - val_dice_coef: 0.4286\n",
      "Epoch 104/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5648 - acc: 0.5050 - dice_coef: 0.5020Epoch 00103: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7564 - acc: 0.5046 - dice_coef: 0.5017 - val_loss: 9.0592 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 105/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8254 - acc: 0.5041 - dice_coef: 0.5013Epoch 00104: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8259 - acc: 0.5041 - dice_coef: 0.5013 - val_loss: 9.0586 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 106/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8842 - acc: 0.5034 - dice_coef: 0.5006Epoch 00105: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7824 - acc: 0.5038 - dice_coef: 0.5010 - val_loss: 9.0587 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 107/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9029 - acc: 0.5028 - dice_coef: 0.5001Epoch 00106: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7823 - acc: 0.5039 - dice_coef: 0.5012 - val_loss: 9.0601 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 108/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9994 - acc: 0.5027 - dice_coef: 0.5000Epoch 00107: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8871 - acc: 0.5033 - dice_coef: 0.5006 - val_loss: 9.0598 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 109/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0788 - acc: 0.5016 - dice_coef: 0.4989Epoch 00108: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9051 - acc: 0.5034 - dice_coef: 0.5006 - val_loss: 9.0597 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 110/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.2077 - acc: 0.5028 - dice_coef: 0.5000Epoch 00109: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9291 - acc: 0.5034 - dice_coef: 0.5006 - val_loss: 9.0601 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 111/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7201 - acc: 0.5049 - dice_coef: 0.5020Epoch 00110: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8006 - acc: 0.5041 - dice_coef: 0.5012 - val_loss: 9.0604 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 112/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7993 - acc: 0.5038 - dice_coef: 0.5010Epoch 00111: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8850 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0606 - val_acc: 0.4298 - val_dice_coef: 0.4286\n",
      "Epoch 113/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7017 - acc: 0.5052 - dice_coef: 0.5024Epoch 00112: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8073 - acc: 0.5042 - dice_coef: 0.5013 - val_loss: 9.0591 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 114/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8538 - acc: 0.5042 - dice_coef: 0.5013Epoch 00113: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8962 - acc: 0.5037 - dice_coef: 0.5009 - val_loss: 9.0595 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 115/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8226 - acc: 0.5038 - dice_coef: 0.5011Epoch 00114: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7827 - acc: 0.5040 - dice_coef: 0.5013 - val_loss: 9.0593 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 116/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7854 - acc: 0.5036 - dice_coef: 0.5008Epoch 00115: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9019 - acc: 0.5035 - dice_coef: 0.5007 - val_loss: 9.0582 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 117/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9283 - acc: 0.5052 - dice_coef: 0.5023Epoch 00116: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8230 - acc: 0.5042 - dice_coef: 0.5013 - val_loss: 9.0590 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 118/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7283 - acc: 0.5047 - dice_coef: 0.5019Epoch 00117: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7956 - acc: 0.5039 - dice_coef: 0.5012 - val_loss: 9.0584 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 119/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8387 - acc: 0.5034 - dice_coef: 0.5006Epoch 00118: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7888 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0583 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 120/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7306 - acc: 0.5043 - dice_coef: 0.5015Epoch 00119: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8155 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0580 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 121/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7123 - acc: 0.5053 - dice_coef: 0.5023Epoch 00120: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8055 - acc: 0.5040 - dice_coef: 0.5011 - val_loss: 9.0592 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 122/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9317 - acc: 0.5033 - dice_coef: 0.5005Epoch 00121: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8450 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0588 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 123/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0831 - acc: 0.5019 - dice_coef: 0.4992Epoch 00122: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8850 - acc: 0.5034 - dice_coef: 0.5007 - val_loss: 9.0594 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 124/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0072 - acc: 0.5030 - dice_coef: 0.5002Epoch 00123: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8141 - acc: 0.5040 - dice_coef: 0.5012 - val_loss: 9.0591 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 125/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0344 - acc: 0.5021 - dice_coef: 0.4992Epoch 00124: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8425 - acc: 0.5038 - dice_coef: 0.5010 - val_loss: 9.0589 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 126/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9298 - acc: 0.5028 - dice_coef: 0.5000Epoch 00125: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8591 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0588 - val_acc: 0.4299 - val_dice_coef: 0.4287\n",
      "Epoch 127/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9209 - acc: 0.5033 - dice_coef: 0.5006Epoch 00126: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9253 - acc: 0.5037 - dice_coef: 0.5009 - val_loss: 9.0583 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 128/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0002 - acc: 0.5034 - dice_coef: 0.5006Epoch 00127: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8412 - acc: 0.5039 - dice_coef: 0.5010 - val_loss: 9.0586 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 129/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6881 - acc: 0.5049 - dice_coef: 0.5020Epoch 00128: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8052 - acc: 0.5040 - dice_coef: 0.5012 - val_loss: 9.0586 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 130/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5958 - acc: 0.5045 - dice_coef: 0.5017Epoch 00129: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8514 - acc: 0.5039 - dice_coef: 0.5011 - val_loss: 9.0595 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 131/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9727 - acc: 0.5035 - dice_coef: 0.5007Epoch 00130: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8803 - acc: 0.5034 - dice_coef: 0.5006 - val_loss: 9.0600 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 132/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0417 - acc: 0.5031 - dice_coef: 0.5004Epoch 00131: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9254 - acc: 0.5030 - dice_coef: 0.5003 - val_loss: 9.0598 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 133/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7193 - acc: 0.5037 - dice_coef: 0.5009Epoch 00132: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8419 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0603 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 134/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7511 - acc: 0.5043 - dice_coef: 0.5014Epoch 00133: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7884 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0601 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 135/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9062 - acc: 0.5028 - dice_coef: 0.5000Epoch 00134: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9094 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0596 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 136/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5955 - acc: 0.5045 - dice_coef: 0.5017Epoch 00135: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7995 - acc: 0.5038 - dice_coef: 0.5011 - val_loss: 9.0605 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 137/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9683 - acc: 0.5033 - dice_coef: 0.5004Epoch 00136: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8014 - acc: 0.5040 - dice_coef: 0.5012 - val_loss: 9.0601 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 138/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9017 - acc: 0.5037 - dice_coef: 0.5009Epoch 00137: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8417 - acc: 0.5039 - dice_coef: 0.5011 - val_loss: 9.0587 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 139/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0374 - acc: 0.5039 - dice_coef: 0.5012Epoch 00138: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8311 - acc: 0.5038 - dice_coef: 0.5010 - val_loss: 9.0594 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 140/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0132 - acc: 0.5033 - dice_coef: 0.5006Epoch 00139: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8501 - acc: 0.5038 - dice_coef: 0.5011 - val_loss: 9.0590 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 141/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0854 - acc: 0.5037 - dice_coef: 0.5009Epoch 00140: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8819 - acc: 0.5039 - dice_coef: 0.5011 - val_loss: 9.0595 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 142/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8963 - acc: 0.5041 - dice_coef: 0.5014Epoch 00141: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8291 - acc: 0.5038 - dice_coef: 0.5011 - val_loss: 9.0594 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 143/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7764 - acc: 0.5038 - dice_coef: 0.5010Epoch 00142: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7279 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0592 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 144/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7921 - acc: 0.5036 - dice_coef: 0.5008Epoch 00143: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8582 - acc: 0.5036 - dice_coef: 0.5008 - val_loss: 9.0593 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 145/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5293 - acc: 0.5047 - dice_coef: 0.5020Epoch 00144: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7602 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0584 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 146/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7725 - acc: 0.5043 - dice_coef: 0.5015Epoch 00145: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7617 - acc: 0.5046 - dice_coef: 0.5018 - val_loss: 9.0581 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 147/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8387 - acc: 0.5039 - dice_coef: 0.5011Epoch 00146: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7579 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0572 - val_acc: 0.4300 - val_dice_coef: 0.4288\n",
      "Epoch 148/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8309 - acc: 0.5037 - dice_coef: 0.5009Epoch 00147: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8894 - acc: 0.5037 - dice_coef: 0.5009 - val_loss: 9.0576 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 149/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8227 - acc: 0.5043 - dice_coef: 0.5016Epoch 00148: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8222 - acc: 0.5038 - dice_coef: 0.5011 - val_loss: 9.0573 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 150/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6553 - acc: 0.5059 - dice_coef: 0.5030Epoch 00149: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8126 - acc: 0.5038 - dice_coef: 0.5010 - val_loss: 9.0570 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 151/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6768 - acc: 0.5042 - dice_coef: 0.5014Epoch 00150: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8816 - acc: 0.5038 - dice_coef: 0.5010 - val_loss: 9.0582 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 152/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9164 - acc: 0.5037 - dice_coef: 0.5009Epoch 00151: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7737 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0581 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 153/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0293 - acc: 0.5029 - dice_coef: 0.5001Epoch 00152: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.9291 - acc: 0.5033 - dice_coef: 0.5005 - val_loss: 9.0581 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 154/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7952 - acc: 0.5041 - dice_coef: 0.5012Epoch 00153: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7934 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0581 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 155/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0523 - acc: 0.5032 - dice_coef: 0.5004Epoch 00154: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8613 - acc: 0.5040 - dice_coef: 0.5012 - val_loss: 9.0573 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 156/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8792 - acc: 0.5043 - dice_coef: 0.5015Epoch 00155: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8198 - acc: 0.5041 - dice_coef: 0.5013 - val_loss: 9.0572 - val_acc: 0.4300 - val_dice_coef: 0.4288\n",
      "Epoch 157/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9012 - acc: 0.5045 - dice_coef: 0.5017Epoch 00156: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7540 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0570 - val_acc: 0.4300 - val_dice_coef: 0.4288\n",
      "Epoch 158/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 44.0206 - acc: 0.5038 - dice_coef: 0.5011Epoch 00157: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8215 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0562 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 159/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9047 - acc: 0.5036 - dice_coef: 0.5008Epoch 00158: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7388 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0569 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 160/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9723 - acc: 0.5035 - dice_coef: 0.5007Epoch 00159: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7577 - acc: 0.5041 - dice_coef: 0.5013 - val_loss: 9.0574 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 161/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7182 - acc: 0.5046 - dice_coef: 0.5018Epoch 00160: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7517 - acc: 0.5047 - dice_coef: 0.5019 - val_loss: 9.0567 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 162/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5480 - acc: 0.5059 - dice_coef: 0.5030Epoch 00161: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7166 - acc: 0.5046 - dice_coef: 0.5018 - val_loss: 9.0567 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 163/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6208 - acc: 0.5047 - dice_coef: 0.5020Epoch 00162: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7848 - acc: 0.5043 - dice_coef: 0.5016 - val_loss: 9.0567 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 164/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6950 - acc: 0.5035 - dice_coef: 0.5007Epoch 00163: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8114 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0571 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 165/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9972 - acc: 0.5023 - dice_coef: 0.4996Epoch 00164: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8137 - acc: 0.5037 - dice_coef: 0.5010 - val_loss: 9.0577 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 166/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6743 - acc: 0.5050 - dice_coef: 0.5021Epoch 00165: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8470 - acc: 0.5039 - dice_coef: 0.5011 - val_loss: 9.0568 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 167/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8089 - acc: 0.5035 - dice_coef: 0.5008Epoch 00166: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8050 - acc: 0.5040 - dice_coef: 0.5013 - val_loss: 9.0570 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 168/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7503 - acc: 0.5052 - dice_coef: 0.5023Epoch 00167: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7615 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0574 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 169/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6752 - acc: 0.5048 - dice_coef: 0.5019Epoch 00168: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7325 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0566 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 170/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7181 - acc: 0.5047 - dice_coef: 0.5019Epoch 00169: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7873 - acc: 0.5041 - dice_coef: 0.5012 - val_loss: 9.0576 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 171/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6634 - acc: 0.5050 - dice_coef: 0.5022Epoch 00170: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7810 - acc: 0.5043 - dice_coef: 0.5015 - val_loss: 9.0560 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 172/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7532 - acc: 0.5041 - dice_coef: 0.5013Epoch 00171: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8335 - acc: 0.5041 - dice_coef: 0.5013 - val_loss: 9.0569 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 173/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6569 - acc: 0.5046 - dice_coef: 0.5018Epoch 00172: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8495 - acc: 0.5038 - dice_coef: 0.5011 - val_loss: 9.0562 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 174/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.4649 - acc: 0.5055 - dice_coef: 0.5026Epoch 00173: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7589 - acc: 0.5045 - dice_coef: 0.5016 - val_loss: 9.0568 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 175/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8115 - acc: 0.5037 - dice_coef: 0.5010Epoch 00174: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7874 - acc: 0.5040 - dice_coef: 0.5013 - val_loss: 9.0567 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 176/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7091 - acc: 0.5048 - dice_coef: 0.5020Epoch 00175: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7525 - acc: 0.5041 - dice_coef: 0.5014 - val_loss: 9.0563 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 177/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9894 - acc: 0.5029 - dice_coef: 0.5002Epoch 00176: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8825 - acc: 0.5038 - dice_coef: 0.5010 - val_loss: 9.0558 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 178/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8426 - acc: 0.5035 - dice_coef: 0.5007Epoch 00177: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8454 - acc: 0.5038 - dice_coef: 0.5011 - val_loss: 9.0561 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 179/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7947 - acc: 0.5033 - dice_coef: 0.5006Epoch 00178: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7500 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0564 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 180/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8488 - acc: 0.5036 - dice_coef: 0.5008Epoch 00179: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7763 - acc: 0.5046 - dice_coef: 0.5017 - val_loss: 9.0563 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 181/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8454 - acc: 0.5046 - dice_coef: 0.5018Epoch 00180: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8219 - acc: 0.5041 - dice_coef: 0.5013 - val_loss: 9.0561 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 182/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9707 - acc: 0.5046 - dice_coef: 0.5018Epoch 00181: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.8225 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0563 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 183/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7589 - acc: 0.5054 - dice_coef: 0.5026Epoch 00182: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7855 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0569 - val_acc: 0.4300 - val_dice_coef: 0.4288\n",
      "Epoch 184/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5811 - acc: 0.5052 - dice_coef: 0.5023Epoch 00183: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7635 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0575 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 185/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8859 - acc: 0.5043 - dice_coef: 0.5015Epoch 00184: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7180 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0583 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 186/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8417 - acc: 0.5049 - dice_coef: 0.5022Epoch 00185: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.6903 - acc: 0.5048 - dice_coef: 0.5020 - val_loss: 9.0576 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 187/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5471 - acc: 0.5053 - dice_coef: 0.5024Epoch 00186: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7994 - acc: 0.5042 - dice_coef: 0.5014 - val_loss: 9.0574 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 188/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9952 - acc: 0.5026 - dice_coef: 0.4998Epoch 00187: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7815 - acc: 0.5041 - dice_coef: 0.5014 - val_loss: 9.0580 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 189/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5617 - acc: 0.5057 - dice_coef: 0.5029Epoch 00188: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7949 - acc: 0.5041 - dice_coef: 0.5014 - val_loss: 9.0570 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 190/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.9130 - acc: 0.5034 - dice_coef: 0.5007Epoch 00189: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7538 - acc: 0.5044 - dice_coef: 0.5017 - val_loss: 9.0565 - val_acc: 0.4301 - val_dice_coef: 0.4288\n",
      "Epoch 191/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7519 - acc: 0.5037 - dice_coef: 0.5009Epoch 00190: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7495 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0574 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 192/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8117 - acc: 0.5043 - dice_coef: 0.5015Epoch 00191: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7824 - acc: 0.5043 - dice_coef: 0.5015 - val_loss: 9.0589 - val_acc: 0.4299 - val_dice_coef: 0.4287\n",
      "Epoch 193/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5474 - acc: 0.5045 - dice_coef: 0.5017Epoch 00192: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7112 - acc: 0.5047 - dice_coef: 0.5019 - val_loss: 9.0586 - val_acc: 0.4299 - val_dice_coef: 0.4287\n",
      "Epoch 194/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7100 - acc: 0.5052 - dice_coef: 0.5024Epoch 00193: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7274 - acc: 0.5046 - dice_coef: 0.5017 - val_loss: 9.0583 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 195/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.7183 - acc: 0.5042 - dice_coef: 0.5015Epoch 00194: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7378 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0575 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 196/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8325 - acc: 0.5039 - dice_coef: 0.5011Epoch 00195: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7790 - acc: 0.5044 - dice_coef: 0.5017 - val_loss: 9.0588 - val_acc: 0.4299 - val_dice_coef: 0.4287\n",
      "Epoch 197/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.6618 - acc: 0.5057 - dice_coef: 0.5028Epoch 00196: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7894 - acc: 0.5044 - dice_coef: 0.5016 - val_loss: 9.0584 - val_acc: 0.4300 - val_dice_coef: 0.4287\n",
      "Epoch 198/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.4040 - acc: 0.5056 - dice_coef: 0.5027Epoch 00197: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7844 - acc: 0.5041 - dice_coef: 0.5011 - val_loss: 9.0589 - val_acc: 0.4299 - val_dice_coef: 0.4287\n",
      "Epoch 199/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.5860 - acc: 0.5058 - dice_coef: 0.5030Epoch 00198: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7727 - acc: 0.5040 - dice_coef: 0.5013 - val_loss: 9.0594 - val_acc: 0.4299 - val_dice_coef: 0.4286\n",
      "Epoch 200/200\n",
      "480/500 [===========================>..] - ETA: 1s - loss: 43.8821 - acc: 0.5043 - dice_coef: 0.5016Epoch 00199: val_loss did not improve\n",
      "500/500 [==============================] - 52s - loss: 43.7387 - acc: 0.5045 - dice_coef: 0.5017 - val_loss: 9.0591 - val_acc: 0.4299 - val_dice_coef: 0.4286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a483ff410>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_images,train_labels,batch_size=20,epochs=200,validation_data=(test_images,test_labels),sample_weight=sample_weights,callbacks=[lr_reducer, csv_logger,model_chekpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_image = train_images[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 64, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_image = np.squeeze(testing_image,axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f09cc738bd0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAAEACAYAAABbOeMfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09cc797dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(testing_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_image = testing_image.reshape((1,216,64,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(testing_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.squeeze(prediction,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13824, 8)\n"
     ]
    }
   ],
   "source": [
    "print(prediction.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction[6999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.reshape(prediction,(216,64,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = np.zeros((216,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground = np.zeros((216,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(216):\n",
    "    for j in range(64):\n",
    "        index = np.argmax(prediction[i][j])\n",
    "        output[i][j] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13824, 8)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[250][6999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ground_truth = np.reshape(train_labels[96],(216,64,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(216):\n",
    "    for j in range(64):\n",
    "        index = np.argmax(test_ground_truth[i][j])\n",
    "        ground[i][j] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(216):\n",
    "    for j in range(64):\n",
    "        index = np.argmax(prediction[i][j])\n",
    "        output[i][j] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 64)\n"
     ]
    }
   ],
   "source": [
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color= np.zeros((216,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "c3 = 0\n",
    "c4 = 0\n",
    "c5 = 0\n",
    "c6 = 0\n",
    "c7 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(216):\n",
    "    for k in range(64):\n",
    "        if(output[j][k]==0):\n",
    "            c0 = c0 + 1\n",
    "            color[j][k] = [0,0,0]\n",
    "        if(output[j][k]==1):\n",
    "            c1 = c1 + 1\n",
    "            color[j][k] = [128,0,0]\n",
    "        if(output[j][k]==2):\n",
    "            c2 = c2 + 1\n",
    "            color[j][k] = [0,128,0]\n",
    "        if(output[j][k]==3):\n",
    "            c3 = c3 + 1\n",
    "            color[j][k] = [128,128,0] \n",
    "        if(output[j][k]==4):\n",
    "            c4 = c4 + 1\n",
    "            color[j][k] = [0,128,128]\n",
    "        if(output[j][k]==5):\n",
    "            c5 = c5 + 1\n",
    "            color[j][k] = [64,0,0]\n",
    "        if(output[j][k]==6):\n",
    "            c6 = c6 + 1\n",
    "            color[j][k] = [192,0,0]\n",
    "        if(output[j][k]==7):\n",
    "            c7 = c7 + 1\n",
    "            color[j][k] = [64,128,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5349\n",
      "2898\n",
      "0\n",
      "1008\n",
      "824\n",
      "2165\n",
      "928\n",
      "652\n"
     ]
    }
   ],
   "source": [
    "print(c0)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)\n",
    "print(c4)\n",
    "print(c5)\n",
    "print(c6)\n",
    "print(c7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f09cc1c30d0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAAEACAYAAABbOeMfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGYIVyhCsUIZghTIEK5QhWKEMwQplCP8PvXIX+JwFhrYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09cc775590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imsave(\"results2.png\",color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color= np.zeros((216,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c0 = 0\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "c3 = 0\n",
    "c4 = 0\n",
    "c5 = 0\n",
    "c6 = 0\n",
    "c7 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(216):\n",
    "    for k in range(64):\n",
    "        if(ground[j][k]==0):\n",
    "            c0 = c0 + 1\n",
    "            color[j][k] = [0,0,0]\n",
    "        if(ground[j][k]==1):\n",
    "            c1 = c1 + 1\n",
    "            color[j][k] = [128,0,0]\n",
    "        if(ground[j][k]==2):\n",
    "            c2 = c2 + 1\n",
    "            color[j][k] = [0,128,0]\n",
    "        if(ground[j][k]==3):\n",
    "            c3 = c3 + 1\n",
    "            color[j][k] = [128,128,0] \n",
    "        if(ground[j][k]==4):\n",
    "            c4 = c4 + 1\n",
    "            color[j][k] = [0,128,128]\n",
    "        if(ground[j][k]==5):\n",
    "            c5 = c5 + 1\n",
    "            color[j][k] = [64,0,0]\n",
    "        if(ground[j][k]==6):\n",
    "            c6 = c6 + 1\n",
    "            color[j][k] = [192,0,0]\n",
    "        if(ground[j][k]==7):\n",
    "            c7 = c7 + 1\n",
    "            color[j][k] = [64,128,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7653\n",
      "338\n",
      "921\n",
      "696\n",
      "538\n",
      "2597\n",
      "652\n",
      "429\n"
     ]
    }
   ],
   "source": [
    "print(c0)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)\n",
    "print(c4)\n",
    "print(c5)\n",
    "print(c6)\n",
    "print(c7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f09cc175590>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAAEACAYAAABbOeMfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEixJREFUeJztnXtwFVWegL/fNYGQABEwhJcJICqKCrgCjihGZRAfK7rl\nKFWOOjKW66irtbu1q9ZOldk/pmrYrVl3t9b5Y3ZY34PjE1EGwSBBkRnBUYeI8ibhkRASgoSERySc\n/eP0TTqXvvf2faX7kPNVdd3uc0/3Ocl3u/t09zm/FqUUlvATCboCFn9YUYZgRRmCFWUIVpQhWFGG\nkDNRIjJXRDaLyFYReTJX5fQVJBfXUSISAbYCNwD1wAZgvlJqc9YL6yPkao+aDmxTStUppb4HXgPm\n5aisPkGuRI0G9riW9zppljSxjQlDyMvRdvcBZa7lMU5aFyJibzI6KKUkWZ5c7VEbgAkiUi4i/YD5\nwNIcldUnyMkepZTqFJHHgJXoH8MipdS3uSirr5CT5rmvgu2hr4sgD32WLGNFGYIVZQhWlCFYUYZg\nRRmCFWUIVpQhWFGGYEUZghVlCFaUIVhRhmBFGYIVZQhWlCFYUYZgRRmCFWUIVpQhWFGGYEUZghVl\nCFaUIVhRhmBFGYIVZQhWlCFYUYZgRRmCFWUIVpQhWFGGYEUZghVlCFaUIVhRhmBFGYIVZQhWlCFY\nUYZgRRmCFWUIGQWtEpFa4DBwCvheKTVdRIYAvwfKgVrgLqXU4Qzr2efJdI86BVQopaYqpaY7aU8B\nVUqpC4GPgKczLMMCoJRKewJ2AcNi0jYDpc78CGBznHWVnfTk53+d6R6lgA9FZIOIPOiklSqlGtE1\n2A8Mz7AMC5kHVpyplGoQkRJgpYhsQctzY+PyZYGM9iilVIPz2QQsQYfRbhSRUgARGQEcyLSSlgxE\niUihiAx05ouAOUANOnbsT5xs9wPvZlhHCxmEKhWRccA76ENbHvCqUuqXIjIUeB04F6hDN8+/81jf\nHhId/IQqtTFlQ4CNKXsGYUUZghVlCFaUIVhRhmBFGYIVZQhWlCFYUYZgRRmCFWUIVpQhWFGGYEUZ\nghVlCFaUIVhRhmBFGYIVZQhWlCFYUYZgRRmCFWUIVpQhWFGGEKiowUC/ICtgEJkOu8mIf3A+nwMO\noocvWrwJVFSUh53PF4HdQVYkxIRC1FnO54/Re9US9PhSSzehEBUler6ah5a3KcC6hI1QtvoGADcD\nTwBXBFyXsBCqPcpNkTNVoH9N6wOtTfCEVlSUgcA1wBTgW+CTYKsTGIGOOCxbAHe/BkVH/a3TBrS4\nlg8A7+egbr1N+IeGVkJ5LeR1ws3LYFhLsrV6chzYhxb4TtZr2HsYISpKeS0UHNfzMz+Fsj3+t9UB\n7AQ6gTeyVsPew4+o0Jyj6sZ2zx8thOJWPX9JDUzcknjdfsBE9DXYnU7ae8CJLNcxSEIjys2eMoju\nUAeHwuaJcN4OuPTrxOtFgEtcy987n5/Q89xmIqEU5aZhlDONhN3lOq10P1zx58TruYUVoM9jnwON\nualmzgm9qCiNI/QEMKQFDg6D4sNw5WfJ173I+RxE9561BR2txBSMEeXm0FD441UwqBWOFum0/A64\nZm3i9Sa65kcAe9GyduSmmlnFSFFRjgyGj2fp+fwOwGk7/WCdbvInYrwz1QGjnLRm9EV1GDFalJvv\n+8GqG5wFBXknYeqXUJCk6VfuTAD7gWJnvh0dgSssJBUlIouAW4FGpdRlTlrcuLEi8jSwADgJPKGU\nWpmbqsdn1Wz92dGv+67H+Vu7m/zxGAHMdeYPA/3R12gbc1LL1Eh6wSsiV6MbTS+5RC0EDiql/k1E\nngSGKKWeEpGLgVeBacAYoAo4X3kUEnvBm2uu+hSGH4Cy3TD0kP/1jgEfuJa/Rl9YZ5OsXPAqpdaK\nSHlM8jzgWmf+RaAaHfT3NuA1pdRJoFZEtqGDLfpom+WWdTP15/TPoNx5jFxyAIY3JV5vAHCHazmC\n3sui1KIPk7km3XPUcHfcWBGJxo0dDfzRlW+fkxYa1s/QE8ClG+ES5yJ6cCuM3J98/Xkxy6vovjZr\nRB8yc0G2GhPp3TBc7ZofC4zLRlX8U3OZngDG74CrP9Xz/Y/D6Hp/27jBNb8O2B7zfSu6NZkp6Ypq\nFJFSpVRjTNzYfejIl1HGOGneXJdm6Tlg53l6An3n49Zlej7S6V/aVc7kZivez9BOAg0p1M/X3XMR\nGQu8p5S61FleCLQopRbGaUzMQB/yPiQkjYl0KWyHe1/umVbSlPw6LRltwCvoS4KsPOYQkd+hn4gP\nQx+Gn0F3FHoDj7ixTvP8p+h7onGb56aI8uK+F2HwkZ5pg1qhf4d3/kRUYtjzKNO56Q9wYZzHMfnf\nx3+KXYkBoooq/eXtoPuRhZsIuvmcjGME2wv3ws1w23ve3/17uwGiKiv95V0LfOyRPhodWD0ZL6Fv\nwII+iYeq63SlYU94E3G1M6XLfa75d4C/ZFadQAhlB8xcMg+4POhKpEGfExUBbkLf1zKJPicKIB99\nR+EHQVckBfqkKNCPMGaR2bmvN+mzokA37f0078NAnxYFMBkz9iojmueJaGEIS3o8MdLMZzGFHEu6\n/kD0jckI3tdqYcFIUXsYQ7Vz6/0E/djb44a95vfcTR6d3ML7DCXxI91B6BuZYcYoUds5jw1Mo42B\n7GNMwrx1jAVgKbdxC8soSfJUaCwwH/3oYU02KptlQitqA1d0/bOjtDCE+hQfGNcyjuXcxI2soDTB\n6xaLnWk0UOJKPwYsS6nE3BAaUdVcy3ec3bW8mzJasnRA2sl5rGQON7CKUUke1w3i9P7r7n/Scnr2\nmegtAhe1ius5zgA2cTFHKcpZOTuYgKCooJoxCR46x5IPTI1JO4k+PLZlUJ8C9EW33701UFGruJ7P\nmEEH/dPextAjR7h73TpfeT+YMoXq0gquZQ3ndt1PT42otHzgKHrgQSojRQYDV6IvuP8KQ0St4yo6\n06hCWVMTP6zR/VjPbm/nr/+cZGiHQ01ZGWtL9ZCBWXxMGSmMlothivNZhO4TkaipMpzuw+lA0rsp\nHKioVCVNaGhgxvbtlDc1MWdj+v1Xt3M+ANfwCeUZxoqZ7HwmE3VpRqWE4ByViCm7djFhf3dnu0l7\n93LdpuyECdnO+Yxhb8aioFtWLgmtqCm7dnH/mjVMra3NWRn1jGIPY9I+X/UmoRQ1ubaWBatXc9nu\n3Iaw2sqFnCKSckswCEIn6tK6Oh6qqmLS3t75lW/nfBTCdawOtazQ3T2/Z+3aXpMUZQcTqGI2+7qG\ntIWPUO1RF9TXU3zUZxiXLFPLOFZwIzexnJH4GC2QBscooOG0H8NOX+uGStSjK1Zw0b7cHX7Kmpsp\nbm/ncJH3HZDdlPM+t3IbSxPeF0yFNoo4xFAAGhjBH7glJkelr+2ERtS5zc0M6MjtXbSHVq3irFOn\neHfaNFoLCz3z7GMM73AHd/Im53Awo/LaKWQD01hDRUbbgRCdo37+1ltc0JDK+Ib0+Onq1dyxfj2D\nEhxi9zOS15hPM+fQwpCUy2inkGbO4U9cmRVJEPAeJUqhRBjW2kp+Z7YHXMbngepq8jo7eW3mTNoL\nCjzzNFPC//AYRbTxEL+hmCQDgB2OUcAarmU9M7JZ5WD3qFEtLZzd3s5/P/884w9k55zgl3s/+YQf\nf/wxBUkOt+0M5DkepZ2irukUPXsgH6d/13dVzM66JAi473l1ICX35M0ZM1h0/fV0RiJ05Of7WucR\nnqPYNQj0Te5kGxekV4HKyvAPEqgOpGRvVl52GQtvvx0FnIr04oHGp6jQtPqCZnZNDddt2sTn48fz\n9D33BF2d07B7VAwnIxGO5+ezs7SUxxcsyH2B9tCXGScjEVoLC2kpKuLBn/0sJ2UMPHaMtoULrahs\n0ClCXUkJCx55JGvbHNXSwq9eeomIUtx9+LAVlS1OibD7nHMAeGzBAtoGpN9jfeyBA/xi8WJGH9Kd\nQivwN+IwNHcmwkxEKcY2NTG2qYn/fOEFhh05knwlDy6or+dfX3+9S1JKdUirxD7MhMZGfrF4MSNS\n/GdP2rOHp5Ysobw5vTguVlQaTKyv5+dvv825Pv/pk2treWLZsozuvthzVAb8pbycIwUFvDJrFptH\ne3e1vnznTh6qqmJivXecngrOoFHxYWVynQ4fPPjYMZoHD/bMM/LQobiSUsGKygK57oQDPs5RIrJI\nRBpFZKMr7RkR2SsiXzjTXNd3T4vINhH5VkTm5KrifQ0/jYnngRs90v9DKXW5M30AICIXAXehQ43f\nBPxaRJIefy3JSSpKKbUWPIfseQmYhxOqVClVC0RDlVoyJJPm+WMi8pWI/FZEolGoR0OPnvehC1Vq\nKumK+jUwXik1BR0b8FfZq5LFi7RafUopd2zj/0W/BQhSDFX6vGt+CqcPGDsT+RL4Ko31/IoSXOck\nERmhlIr2UvwbdDhwgKXAqyLyLPqQN4EE75F8IOXqms9Uev4gX/S5np83CXSFKhWR3ehQpdeJyBR0\n6Lta4G8BlFLfiMjrwDfo4a+PeMWTtaSOvYUUMBXYxxxnFFaUIVhRhmBFGYIVZQhWlCFYUYZgRRmC\nFWUIVpQhWFGGYEUZghVlCFaUIVhRhmBFGYIVZQiBdml+M8X849ABc/sigYr6OnmWHjRABuF6dU/Q\n8AZ6S4xRgwQOOlO6tEEakY28mYUOZt9bGCUqU2Lf454JHYB3fLLc0KdEZZPefvOobfUZghVlCFaU\nIVhRhhBoY2LSpNPT6uqgLZP3/ZyhBCrqRz86Pa26GvwOIj9+HHphnHMoCF3zvKLCf96mJli+3H/+\n/fshoLDqGRM6UalQUgL33ec/f1WVPrQm48QJ6OUQt0kxWlSqzJ7tL19DA7z7bmrb/u47fSjOFX1K\nlF9GjoSHH05tnRUrYNu2NArzGcMq2IFslYEUHSoqKg0YyHbKDhr1TaCHvuZWGOgdyB+A/DzIP6v3\n6hNmAhV117OJv/+7uXB7CnFfzjqD77MEeo5Klici4DdW/NmF8OY/Zlqr3sfvOSrUolKlKP33LgdG\n+4k+KMpUQt/qs/jHijIEK8oQrChD8BNTdoyIfCQim0SkRkQed9KHiMhKEdkiIitcwRVtXNlcoJRK\nOAEjgCnO/EBgCzARWAj8s5P+JPBLZ/5idFi6PGAsujudeGxX2UlPyRwopZKL8vgHLwFmA5uBUpfM\nzc78U8CTrvzLgRlWVGaiUjpHichYdLDKPzmSGtEl7QeGO9lsXNkc4FuUiAxED8B4QinVhv41uIld\ntmQRX6JEJA8t6WWlVPTZZ6OIlDrfj4Cud3anFFfW4g+/e9T/Ad8opf7LlbYU+Ikzfz/writ9voj0\nE5FxJIkra/GJj8bDTKATHVz4S+ALYC4wFKhCtwJXAme71nka3dr7FpgTZ7uBn8TDMvlpTNibsiHA\n3pQ9g7CiDMGKMgQryhCsKEOwogzBijIEK8oQrChDsKIMwYoyBCvKEKwoQwjs7rklNeweZQhWlCmk\n2l0sGxP6CfFmYCuurmXOd4uARmCjK20I+inyFmAFUIzui/ERsAmoAR6Pk7cE+Az9dLoGeCbeNp30\nCPop9tIk+WrR0eC+BNYnqGcx8Ab6afcmYEa8bWa1X18WJEXQj+nLgXz0I/6Jru+vRndJc4s6rbMn\nKXQMBQqd5bPQXd2me+Vz5v8eeMUlKl6+ncCQmL/Nq+wXgAectDxHnOc2wybqSmC5a7lHh00nrTxG\nlGdnz5h1EnYMdZYLgc+BaV750Hvph+i3ri5NVDawCxgWU4fYvFuBHR51Tfr3xE5BnKNiO2juJXkH\nzeHKu7MnkLxjqIhERORL9HvtP1RKbfDKBzwL/BM9+yjG62iqgA9FZIOIPBgnbynQLCLPi8gXIvIb\nESlMsM24mNqY6PpH+ukYqpQ6pZSait5jpovIJI98eUCjUuorXK9bT1D2TKXU5cDNwKMico1X2cDl\nwHNO3nb0EcQrX0KCELUPKHMt++mg6dnZM8WOoSilWoFqdGMmNl8HcJuI7AQWA9eLyMvAfq/tKaUa\nnM8m9GF3epyy9yilPneq8BZaXNw6xiMIURuACSJSLiL9gPnoTptuhJ6/6nidPf10DF0ZHRIkIgOA\nH6JbYLH5fquUKlNKjXfq9JFS6l7gvdiyRaTQ2ZMRkSJgDrpFGbvNt4E9InKBk3YDuuUX7++JT283\nJpwT6Fx0K20b8FTMd78D6oETwG7gAXRztkdnT/x3DL3K+e4rYCPwL045iTqQXkt3Y+K0fOiXGkTL\nrYn+DXHyTkb/OL9yxBUnKjveZG8hGYKpjYk+hxVlCFaUIVhRhmBFGYIVZQhWlCFYUYbw/0D/ggCA\niPedAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09cc6a3a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imsave(\"results.png\",color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
